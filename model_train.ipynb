{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ba519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ec37c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed44b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------\n",
    "#               Basic blocks\n",
    "# -----------------------------------------------\n",
    "class REBNCONV(nn.Module):\n",
    "    \"\"\"ReLU + BN + 3x3 Conv (used throughout U²-Net).\"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        padding = dilation\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "# -----------------------------------------------\n",
    "#          Residual U-blocks (RSU)\n",
    "# -----------------------------------------------\n",
    "\n",
    "def _upsample_like(src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Bilinear upsample `src` to the spatial size of `tgt`.\"\"\"\n",
    "    return F.interpolate(src, size=tgt.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "class _RSU_Base(nn.Module):\n",
    "    \"\"\"Base class for RSU blocks with variable depth.\"\"\"\n",
    "    def __init__(self, height: int, in_ch: int, mid_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        # initial conv\n",
    "        self.rebn_in = REBNCONV(in_ch, out_ch)\n",
    "\n",
    "        # encoder convs\n",
    "        self.enc = nn.ModuleList()\n",
    "        self.pool = nn.ModuleList()\n",
    "        for _ in range(height - 1):\n",
    "            self.enc.append(REBNCONV(out_ch if _ == 0 else mid_ch, mid_ch))\n",
    "            self.pool.append(nn.MaxPool2d(2, stride=2, ceil_mode=True))\n",
    "\n",
    "        # bottom conv (dilated)\n",
    "        self.btm = REBNCONV(mid_ch, mid_ch, dilation=2)\n",
    "\n",
    "        # decoder convs\n",
    "        self.dec = nn.ModuleList()\n",
    "        for _ in range(height - 1):\n",
    "            self.dec.append(REBNCONV(mid_ch * 2, mid_ch))\n",
    "\n",
    "        # final conv\n",
    "        self.rebn_out = REBNCONV(mid_ch + out_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = self.rebn_in(x)\n",
    "\n",
    "        # encoder\n",
    "        enc_feats = []\n",
    "        h = x_in\n",
    "        for enc, pool in zip(self.enc, self.pool):\n",
    "            h = enc(h)\n",
    "            enc_feats.append(h)\n",
    "            h = pool(h)\n",
    "\n",
    "        # bottom\n",
    "        h = self.btm(h)\n",
    "\n",
    "        # decoder\n",
    "        for idx, dec in enumerate(reversed(self.dec)):\n",
    "            h = _upsample_like(h, enc_feats[-(idx + 1)])\n",
    "            h = dec(torch.cat([h, enc_feats[-(idx + 1)]], dim=1))\n",
    "\n",
    "        # output\n",
    "        h = self.rebn_out(torch.cat([h, x_in], dim=1))\n",
    "        return h + x_in  # residual\n",
    "\n",
    "\n",
    "class RSU7(_RSU_Base):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super().__init__(height=7, in_ch=in_ch, mid_ch=mid_ch, out_ch=out_ch)\n",
    "\n",
    "\n",
    "class RSU6(_RSU_Base):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super().__init__(height=6, in_ch=in_ch, mid_ch=mid_ch, out_ch=out_ch)\n",
    "\n",
    "\n",
    "class RSU5(_RSU_Base):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super().__init__(height=5, in_ch=in_ch, mid_ch=mid_ch, out_ch=out_ch)\n",
    "\n",
    "\n",
    "class RSU4(_RSU_Base):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super().__init__(height=4, in_ch=in_ch, mid_ch=mid_ch, out_ch=out_ch)\n",
    "\n",
    "\n",
    "class RSU4F(nn.Module):\n",
    "    \"\"\"RSU4F: RSU block without pooling (all convolutions with dilation).\"\"\"\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.rebn_in = REBNCONV(in_ch, out_ch)\n",
    "\n",
    "        self.enc1 = REBNCONV(out_ch, mid_ch)\n",
    "        self.enc2 = REBNCONV(mid_ch, mid_ch, dilation=2)\n",
    "        self.enc3 = REBNCONV(mid_ch, mid_ch, dilation=4)\n",
    "\n",
    "        self.dec1 = REBNCONV(mid_ch * 2, mid_ch, dilation=2)\n",
    "        self.dec2 = REBNCONV(mid_ch * 2, mid_ch, dilation=1)\n",
    "\n",
    "        self.rebn_out = REBNCONV(mid_ch + out_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = self.rebn_in(x)\n",
    "\n",
    "        h1 = self.enc1(x_in)\n",
    "        h2 = self.enc2(h1)\n",
    "        h3 = self.enc3(h2)\n",
    "\n",
    "        d1 = self.dec1(torch.cat([h3, h2], dim=1))\n",
    "        d2 = self.dec2(torch.cat([d1, h1], dim=1))\n",
    "\n",
    "        h = self.rebn_out(torch.cat([d2, x_in], dim=1))\n",
    "\n",
    "        return h + x_in\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "#                U²-Net Model\n",
    "# -----------------------------------------------\n",
    "class U2Net_Hierarchical(nn.Module):\n",
    "    \"\"\"\n",
    "    U²-Net for hierarchical semantic segmentation.\n",
    "    Args:\n",
    "        num_classes (int): Number of output masks (channels) to predict.\n",
    "    Input:\n",
    "        RGB image tensor of shape (B, 3, H, W)\n",
    "    Output:\n",
    "        Tensor of shape (B, num_classes, H, W) -- hierarchical masks\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 9, base_ch: int = 64):\n",
    "        super().__init__()\n",
    "        self.stage1 = RSU7(3, base_ch, base_ch)          # 3 -> 64\n",
    "        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.stage2 = RSU6(base_ch, base_ch, base_ch)\n",
    "        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.stage3 = RSU5(base_ch, base_ch, base_ch)\n",
    "        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.stage4 = RSU4(base_ch, base_ch, base_ch)\n",
    "        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.stage5 = RSU4F(base_ch, base_ch, base_ch)\n",
    "        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.stage6 = RSU4F(base_ch, base_ch, base_ch)\n",
    "\n",
    "        # decoder\n",
    "        self.stage5d = RSU4F(base_ch * 2, base_ch, base_ch)\n",
    "        self.stage4d = RSU4(base_ch * 2, base_ch, base_ch)\n",
    "        self.stage3d = RSU5(base_ch * 2, base_ch, base_ch)\n",
    "        self.stage2d = RSU6(base_ch * 2, base_ch, base_ch)\n",
    "        self.stage1d = RSU7(base_ch * 2, base_ch, base_ch)\n",
    "\n",
    "        # side output convolutions (produce feature maps prior to final 1x1)\n",
    "        self.side1 = nn.Conv2d(base_ch, num_classes, kernel_size=3, padding=1)\n",
    "        self.side2 = nn.Conv2d(base_ch, num_classes, kernel_size=3, padding=1)\n",
    "        self.side3 = nn.Conv2d(base_ch, num_classes, kernel_size=3, padding=1)\n",
    "        self.side4 = nn.Conv2d(base_ch, num_classes, kernel_size=3, padding=1)\n",
    "        self.side5 = nn.Conv2d(base_ch, num_classes, kernel_size=3, padding=1)\n",
    "        self.side6 = nn.Conv2d(base_ch, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(num_classes * 6, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        hx1 = self.stage1(x)    # (B, C, H, W)\n",
    "        hx = self.pool12(hx1)\n",
    "\n",
    "        hx2 = self.stage2(hx)\n",
    "        hx = self.pool23(hx2)\n",
    "\n",
    "        hx3 = self.stage3(hx)\n",
    "        hx = self.pool34(hx3)\n",
    "\n",
    "        hx4 = self.stage4(hx)\n",
    "        hx = self.pool45(hx4)\n",
    "\n",
    "        hx5 = self.stage5(hx)\n",
    "        hx = self.pool56(hx5)\n",
    "\n",
    "        hx6 = self.stage6(hx)\n",
    "\n",
    "        # Decoder\n",
    "        hx5d = self.stage5d(torch.cat([_upsample_like(hx6, hx5), hx5], dim=1))\n",
    "        hx4d = self.stage4d(torch.cat([_upsample_like(hx5d, hx4), hx4], dim=1))\n",
    "        hx3d = self.stage3d(torch.cat([_upsample_like(hx4d, hx3), hx3], dim=1))\n",
    "        hx2d = self.stage2d(torch.cat([_upsample_like(hx3d, hx2), hx2], dim=1))\n",
    "        hx1d = self.stage1d(torch.cat([_upsample_like(hx2d, hx1), hx1], dim=1))\n",
    "\n",
    "        # Side outputs\n",
    "        d1 = self.side1(hx1d)\n",
    "        d2 = self.side2(hx2d)\n",
    "        d3 = self.side3(hx3d)\n",
    "        d4 = self.side4(hx4d)\n",
    "        d5 = self.side5(hx5d)\n",
    "        d6 = self.side6(hx6)\n",
    "\n",
    "        d1 = _upsample_like(d1, x)\n",
    "        d2 = _upsample_like(d2, x)\n",
    "        d3 = _upsample_like(d3, x)\n",
    "        d4 = _upsample_like(d4, x)\n",
    "        d5 = _upsample_like(d5, x)\n",
    "        d6 = _upsample_like(d6, x)\n",
    "\n",
    "        # Fusion\n",
    "        d0 = self.out_conv(torch.cat([d1, d2, d3, d4, d5, d6], dim=1))\n",
    "\n",
    "        # Output is a tensor (B, num_classes, H, W). For compatibility we also\n",
    "        # return the side outputs (each num_classes channels).\n",
    "        return d0, (d1, d2, d3, d4, d5, d6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c9877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = U2Net_Hierarchical(num_classes=9)\n",
    "x = torch.randn(2, 3, 256, 256)\n",
    "with torch.no_grad():\n",
    "    y, sides = model(x)\n",
    "print(y.shape)  # Expected: (2, 9, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a382b",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Example dataset that yields (image, mask) tuples.\n",
    "\n",
    "    * **image**  – (3, H, W) float32 tensor scaled to [0, 1].\n",
    "    * **mask**   – (H, W) long tensor with class indices.\n",
    "\n",
    "    The dataset must already encode background as a unique class index\n",
    "    (e.g. 0). See `LEVELS` below for the mapping used here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root: str | Path, split: str = \"train\") -> None:\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        # TODO: implement your image/mask listing logic here\n",
    "        self.items: List[Path] = sorted((self.root / split).glob(\"*.png\"))\n",
    "\n",
    "        self.image_tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path = self.items[idx]\n",
    "        # Replace the following two lines with proper I/O according to your\n",
    "        # dataset format (e.g. using PIL.Image or cv2).\n",
    "        from PIL import Image\n",
    "        image = self.image_tf(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = torch.as_tensor(Image.open(img_path.with_suffix(\".label.png\")), dtype=torch.long)\n",
    "        return image, mask\n",
    "\n",
    "###############################################################################\n",
    "#                         Hierarchy definition                                #\n",
    "###############################################################################\n",
    "\n",
    "# Channel indices in model output / ground‑truth label IDs\n",
    "LEVELS: list[list[int]] = [\n",
    "    [1],                              # Level‑0 – body (ignore background 0)\n",
    "    [2, 3],                           # Level‑1 – upper_body, lower_body\n",
    "    [4, 5, 6, 7, 8, 9],               # Level‑2 – fine parts\n",
    "]\n",
    "\n",
    "BACKGROUND_ID = 0\n",
    "IGNORE_INDEX = 255  # value used in CrossEntropyLoss to ignore pixels\n",
    "\n",
    "def _prepare_level_target(gt: torch.Tensor, cls_ids: list[int]) -> torch.Tensor:\n",
    "    \"\"\"Return **remapped** GT where *cls_ids* become {0, …, K‑1}; others→IGNORE.\"\"\"\n",
    "    remapped = torch.full_like(gt, IGNORE_INDEX)\n",
    "    for new_idx, cid in enumerate(cls_ids):\n",
    "        remapped[gt == cid] = new_idx\n",
    "    return remapped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25822b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81cce46b",
   "metadata": {},
   "source": [
    "# Losses and metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7960d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hierarchical_losses(pred: torch.Tensor, gt: torch.Tensor) -> list[torch.Tensor]:\n",
    "    \"\"\"Compute CE loss at each hierarchy level.\n",
    "\n",
    "    Args:\n",
    "        pred: (B, 9, H, W) logits from U²‑Net (no background channel).\n",
    "        gt:   (B, H, W) ground‑truth with class IDs incl. background.\n",
    "    Returns:\n",
    "        List with three scalar losses [loss0, loss1, loss2].\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for cls_ids in LEVELS:\n",
    "        tgt = _prepare_level_target(gt, cls_ids)            # (B, H, W)\n",
    "        logits = pred[:, cls_ids, :, :]                     # (B, K, H, W)\n",
    "        losses.append(F.cross_entropy(logits, tgt, ignore_index=IGNORE_INDEX))\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_mIoU(pred: torch.Tensor, gt: torch.Tensor, cls_ids: list[int]) -> float:\n",
    "    \"\"\"Mean IoU for given *cls_ids*, ignoring background.\n",
    "\n",
    "    Args:\n",
    "        pred: (B, 9, H, W) logits.\n",
    "        gt:   (B, H, W) labels.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        preds = pred.argmax(dim=1)  # (B, H, W)\n",
    "        ious = []\n",
    "        for cid in cls_ids:\n",
    "            pred_mask = preds == cid\n",
    "            true_mask = gt == cid\n",
    "            intersection = (pred_mask & true_mask).sum().item()\n",
    "            union = (pred_mask | true_mask).sum().item()\n",
    "            if union == 0:\n",
    "                continue  # class absent in both → skip from mean\n",
    "            ious.append(intersection / union)\n",
    "        return float(sum(ious) / max(len(ious), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0c25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f0c50d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60117806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: nn.Module, loader: DataLoader, optim: torch.optim.Optimizer, device: torch.device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for imgs, gts in tqdm(loader, desc=\"train\", leave=False):\n",
    "        imgs, gts = imgs.to(device, non_blocking=True), gts.to(device, non_blocking=True)\n",
    "        logits, _ = model(imgs)\n",
    "\n",
    "        l0, l1, l2 = hierarchical_losses(logits, gts)\n",
    "        loss = l0 + l1 + l2\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss.item() * imgs.size(0)\n",
    "    return epoch_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    tot = {k: 0.0 for k in range(3)}\n",
    "    with torch.no_grad():\n",
    "        for imgs, gts in tqdm(loader, desc=\"eval\", leave=False):\n",
    "            imgs, gts = imgs.to(device, non_blocking=True), gts.to(device, non_blocking=True)\n",
    "            logits, _ = model(imgs)\n",
    "            for lvl, cls_ids in enumerate(LEVELS):\n",
    "                tot[lvl] += compute_mIoU(logits, gts, cls_ids) * imgs.size(0)\n",
    "    return {f\"mIoU^{lvl}\": tot[lvl] / len(loader.dataset) for lvl in range(3)}\n",
    "\n",
    "\n",
    "parser.add_argument(\"--data\", type=str, required=True, help=\"Path to dataset root\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=80)\n",
    "parser.add_argument(\"--batch\", type=int, default=4)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--save\", type=str, default=\"checkpoints\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = U2Net_Hierarchical(num_classes=9).to(device)\n",
    "\n",
    "# Data\n",
    "train_ds = SegmentationDataset(args.data, split=\"train\")\n",
    "val_ds = SegmentationDataset(args.data, split=\"val\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optimiser\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=args.epochs)\n",
    "\n",
    "save_dir = Path(args.save); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optim, device)\n",
    "    metrics = evaluate(model, val_loader, device)\n",
    "    lr_sched.step()\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Epoch {epoch:03d} | loss={train_loss:.4f} | \" + \", \".join(f\"{k}={v:.3f}\" for k, v in metrics.items()))\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optim\": optim.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }, save_dir / f\"checkpoint_{epoch:03d}.pth\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
