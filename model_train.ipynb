{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ba519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "from hierarchical_semantic_segmentation.model import U2Net_Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a382b",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Example dataset that yields (image, mask) tuples.\n",
    "\n",
    "    * **image**  – (3, H, W) float32 tensor scaled to [0, 1].\n",
    "    * **mask**   – (H, W) long tensor with class indices.\n",
    "\n",
    "    The dataset must already encode background as a unique class index\n",
    "    (e.g. 0). See `levels` below for the mapping used here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                root_path = 'hierarchical_semantic_segmentation/Pascal-part',\n",
    "                images_folder = 'JPEGImages', \n",
    "                masks_folder='gt_masks', \n",
    "                split='train',\n",
    "                num_classes=7,\n",
    "                resize_shape=(256, 256)):\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.resize_shape = resize_shape\n",
    "        self.root = Path(root_path)\n",
    "        self.images_paths = list((self.root).glob(f\"{images_folder}/*.jpg\"))\n",
    "        self.images_paths_dict={ path.name.split('.')[0]:path  for path in self.images_paths}\n",
    "\n",
    "        self.masks_paths = list((self.root).glob(f\"{masks_folder}/*.npy\"))\n",
    "        self.masks_paths_dict={ path.name.split('.')[0]:path  for path in self.masks_paths}\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize(self.resize_shape, interpolation=Image.NEAREST),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        if split=='train':\n",
    "            with open(root_path+'/train_id.txt') as file:\n",
    "                img_names=file.readlines()\n",
    "                self.img_names = [line[:-1] for line in img_names]\n",
    "            \n",
    "            tmp_names = []\n",
    "            for name in self.img_names:\n",
    "                if name in list(self.images_paths_dict.keys()) and name in list(self.masks_paths_dict.keys()):\n",
    "                    tmp_names.append(name)\n",
    "\n",
    "            self.img_names=copy.copy(tmp_names)\n",
    "\n",
    "        elif  split=='val':\n",
    "            with open(root_path+'/val_id.txt') as file:\n",
    "                img_names=file.readlines()\n",
    "                self.img_names = [line[:-1] for line in img_names]\n",
    "            \n",
    "            tmp_names = []\n",
    "            for name in self.img_names:\n",
    "                if name in list(self.images_paths_dict.keys()) and name in list(self.masks_paths_dict.keys()):\n",
    "                    tmp_names.append(name)\n",
    "\n",
    "            self.img_names=copy.copy(tmp_names)\n",
    "        else:\n",
    "            raise Exception('error in split')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_names = self.img_names[idx]\n",
    "\n",
    "        img_path = self.images_paths_dict[img_names]\n",
    "        image = self.image_transform(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        masks_path = self.masks_paths_dict[img_names]\n",
    "        mask = np.load(masks_path)\n",
    "        mask = transform.resize(mask,\n",
    "                                self.resize_shape,\n",
    "                                preserve_range=True,\n",
    "                                order=0,\n",
    "                                anti_aliasing=False).astype(np.uint8)\n",
    "\n",
    "        one_hot_mask = np.eye(self.num_classes)[mask] \n",
    "        one_hot_mask = np.moveaxis(one_hot_mask, -1, 0) \n",
    "\n",
    "        one_hot_mask = torch.as_tensor(one_hot_mask, dtype=torch.long)\n",
    "        mask = torch.as_tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        # print('\\nid=',idx,'img_names',img_names, 'img_shape',image.shape,'mask_shape',mask.shape)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce45a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'hierarchical_semantic_segmentation/Pascal-part'\n",
    "dataset = SegmentationDataset(root_path=root_path, split='train', resize_shape=(64,64))\n",
    "# dataset = SegmentationDataset(root_path=root_path, split='train')\n",
    "print(dataset.__len__())\n",
    "\n",
    "plt.imshow(dataset[2][1],cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0c50d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d005747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optim, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for imgs, gts in tqdm(loader, desc=\"train\", leave=False):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        gts =  gts.to(device, non_blocking=True)\n",
    "        \n",
    "        logits, _ = model(imgs)\n",
    "\n",
    "        loss = hierarchical_losses(logits, gts)\n",
    "        loss = loss[0]\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss.item() * imgs.size(0)\n",
    "    return epoch_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device):\n",
    "    model.eval()\n",
    "    levels = [[1, 2, 3, 4, 5, 6],[1,6,2,4],[3,5],[1],[6],[2],[4],[3],[5]]\n",
    "    tot = {k: 0.0 for k in range(len(levels))}\n",
    "\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, gts in tqdm(loader, desc=\"eval\", leave=False):\n",
    "            imgs, gts = imgs.to(device, non_blocking=True), gts.to(device, non_blocking=True)\n",
    "            logits, _ = model(imgs)\n",
    "            for lvl, cls_ids in enumerate(levels):\n",
    "                tot[lvl] += compute_mIoU(logits, gts, cls_ids) * imgs.size(0)\n",
    "            \n",
    "\n",
    "            loss = hierarchical_losses(logits, gts)\n",
    "            loss = loss[0]\n",
    "\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    return val_loss/len(loader.dataset), {f\"mIoU^{lvl}\": tot[lvl] / len(loader.dataset) for lvl in range(len(levels))}\n",
    "\n",
    "def _prepare_level_target(gt: torch.Tensor, cls_ids: list[int]) -> torch.Tensor:\n",
    "    remapped = torch.full_like(gt, 255)\n",
    "    for new_idx, cid in enumerate(cls_ids):\n",
    "        remapped[gt == cid] = new_idx\n",
    "    return remapped\n",
    "\n",
    "\n",
    "def hierarchical_losses(pred, gt):\n",
    "    \"\"\"Compute CE loss at each hierarchy level.\n",
    "\n",
    "    Args:\n",
    "        pred: (B, 9, H, W) logits from U²‑Net (no background channel).\n",
    "        gt:   (B, H, W) ground‑truth with class IDs incl. background.\n",
    "    Returns:\n",
    "        List with three scalar losses [loss0, loss1, loss2].\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    levels = [[1, 2, 3, 4, 5, 6],[1,6,2,4],[3,5],[1],[6],[2],[4],[3],[5]]\n",
    "    \n",
    "\n",
    "    for cls_ids in levels:\n",
    "        tgt = _prepare_level_target(gt, cls_ids)            # (B, H, W)\n",
    "        logits = pred[:, cls_ids, :, :]                     # (B, K, H, W)\n",
    "        losses.append(F.cross_entropy(logits, tgt, ignore_index=255))\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_mIoU(pred: torch.Tensor, gt: torch.Tensor, cls_ids: list[int]) -> float:\n",
    "    \"\"\"Mean IoU for given *cls_ids*, ignoring background.\n",
    "\n",
    "    Args:\n",
    "        pred: (B, 9, H, W) logits.\n",
    "        gt:   (B, H, W) labels.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        preds = pred.argmax(dim=1)  # (B, H, W)\n",
    "        ious = []\n",
    "        for cid in cls_ids:\n",
    "            pred_mask = preds == cid\n",
    "            true_mask = gt == cid\n",
    "            intersection = (pred_mask & true_mask).sum().item()\n",
    "            union = (pred_mask | true_mask).sum().item()\n",
    "            if union == 0:\n",
    "                continue  # class absent in both → skip from mean\n",
    "            ious.append(intersection / union)\n",
    "        return float(sum(ious) / max(len(ious), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be24aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U2Net_Hierarchical(num_classes=7)\n",
    "# x = torch.randn(2, 3, 128, 128)\n",
    "# x = torch.randn(2, 3, 100, 100)\n",
    "x = torch.randn(2, 3, 64, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y, sides = model(x)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to dataset root\n",
    "root_path = 'hierarchical_semantic_segmentation/Pascal-part'\n",
    "#path to save checkpoints\n",
    "save = \"hierarchical_semantic_segmentation/checkpoints\"\n",
    "\n",
    "epochs = 100\n",
    "batch =128\n",
    "lr = 1e-3\n",
    "# device='cpu'\n",
    "device='cuda'\n",
    "\n",
    "# resize_shape = (128,128)\n",
    "# resize_shape = (100,100)\n",
    "resize_shape = (64,64)\n",
    "\n",
    "# Model\n",
    "model = U2Net_Hierarchical(num_classes=9).to(device)\n",
    "\n",
    "# Data\n",
    "train_ds = SegmentationDataset(root_path, split=\"train\",resize_shape=resize_shape)\n",
    "val_ds = SegmentationDataset(root_path, split=\"val\",resize_shape=resize_shape)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optimiser\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs)\n",
    "\n",
    "save_dir = Path(save); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c21c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_value(val):\n",
    "    if isinstance(val, tuple):\n",
    "        return tuple(round(x, 3) for x in val)\n",
    "    return round(val, 3)\n",
    "\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    train_loss = train_one_epoch(model, train_loader, optim, device)\n",
    "    \n",
    "    val_loss, metrics = evaluate(model, val_loader, device)\n",
    "    \n",
    "    lr_sched.step()\n",
    "\n",
    "    result = {\n",
    "    'mIoU0': metrics['mIoU^0']/6,\n",
    "    'mIoU^1': (metrics['mIoU^1'] + metrics['mIoU^6'] + metrics['mIoU^2'] + metrics['mIoU^4']/4,\n",
    "               metrics['mIoU^4'] + metrics['mIoU^5']/2),\n",
    "\n",
    "    'mIoU^2': (metrics['mIoU^1'], metrics['mIoU^6'], metrics['mIoU^2'],  \n",
    "              metrics['mIoU^4'], metrics['mIoU^3'], metrics['mIoU^5'])\n",
    "              }\n",
    "    \n",
    "    result = {k: round_value(v) for k, v in result.items()}\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Epoch {epoch:03d} | loss={train_loss:.4f} | val_loss={val_loss:.4f} | metrics={result}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optim\": optim.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }, save_dir / f\"checkpoint_{epoch:05d}.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c615f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_loader, device, num_images=4):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on test images\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: Test DataLoader\n",
    "        device: Device (cuda/cpu)\n",
    "        num_images: Number of images to visualize\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:num_images]\n",
    "    labels = labels[:num_images]\n",
    "    \n",
    "    # Move to device and predict\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        logits, _ = model(images)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "    \n",
    "    # Convert images and labels to numpy\n",
    "    images = images.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(8, 3*num_images))\n",
    "    if num_images == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    \n",
    "    # Define class names for visualization (modify according to your classes)\n",
    "    class_names = {\n",
    "        0: \"Background\",\n",
    "        1: \"Class1\",\n",
    "        2: \"Class2\",\n",
    "        3: \"Class3\",\n",
    "        4: \"Class4\",\n",
    "        5: \"Class5\",\n",
    "        6: \"Class6\",\n",
    "        7: \"Class7\",\n",
    "        8: \"Class8\"\n",
    "    }\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Original image\n",
    "        img = np.transpose(images[i], (1, 2, 0))  # CHW to HWC\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0,1]\n",
    "        \n",
    "        # Ground truth\n",
    "        gt = labels[i]\n",
    "        \n",
    "        # Prediction\n",
    "        pred = preds[i]\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(\"Input Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(gt, cmap='jet', vmin=0, vmax=8)\n",
    "        axes[i, 1].set_title(\"Ground Truth\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred, cmap='jet', vmin=0, vmax=8)\n",
    "        axes[i, 2].set_title(\"Prediction\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to dataset root\n",
    "root_path = 'hierarchical_semantic_segmentation/Pascal-part'\n",
    "#path to save checkpoints\n",
    "\n",
    "# device='cpu'\n",
    "device='cuda'\n",
    "\n",
    "# resize_shape = (128,128)\n",
    "resize_shape = (100,100)\n",
    "\n",
    "val_ds = SegmentationDataset(root_path, split=\"val\",resize_shape=resize_shape)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "visualize_predictions(model, val_loader, device, num_images=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
